{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Author: Yagnik Poshiya\n",
    "GitHub: @yagnikposhiya\n",
    "U & PU. Patel Department of Computer Engineering,\n",
    "Chandubhai S. Patel Institute of Technology,\n",
    "Charotar University of Science and Technology,\n",
    "Changa-388421, Anand, Gujarat, India.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import matplotlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D, GlobalAveragePooling2D, Dense, MaxPool2D\n",
    "from keras import Model, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_one(ORG_IMG):\n",
    "    return cv2.flip(ORG_IMG,1)\n",
    "\n",
    "def transform_two(ORG_IMG):\n",
    "    return cv2.flip(ORG_IMG,0)\n",
    "\n",
    "def transform_three(ORG_IMG):\n",
    "    return cv2.rotate(ORG_IMG, cv2.ROTATE_180)\n",
    "\n",
    "\n",
    "def preprocessingFunction(IMG):\n",
    "    TRANSFORM_FLIP_1 = transform_one(IMG)\n",
    "    TRANSFORM_FLIP_0 = transform_two(TRANSFORM_FLIP_1)\n",
    "    ROTATE_180_DEGREE = transform_three(TRANSFORM_FLIP_0)\n",
    "    RESIZED_IMG = cv2.resize(ROTATE_180_DEGREE, (224,224))\n",
    "    return RESIZED_IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = ImageDataGenerator(samplewise_center=True,\n",
    "                                   rescale=1./255,\n",
    "                                   fill_mode='nearest',\n",
    "                                   brightness_range=[0.4,1.5],\n",
    "                                   rotation_range=10,\n",
    "                                   zoom_range=0.1,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   preprocessing_function=preprocessingFunction) # generate the object for the ImageDataGenerator class for training set\n",
    "datagen_valid = ImageDataGenerator(samplewise_center=True) # generate the object for the ImageDataGenerator class for validation set\n",
    "\n",
    "train_set = datagen_train.flow_from_directory('/path_to_train_directory/train/',\n",
    "                                              target_size=(224,224),\n",
    "                                              color_mode=\"rgb\",\n",
    "                                              class_mode=\"categorical\",\n",
    "                                              batch_size=32,\n",
    "                                              save_to_dir='/path_to_augmented_directory/_augmented_/',\n",
    "                                              save_prefix='aug',\n",
    "                                              save_format='png') # \"class_mode\" is \"categorical\" because here the problem is multi-class problem.\n",
    "validation_set = datagen_valid.flow_from_directory('/path_to_validation_directory/validation/',\n",
    "                                                   target_size=(224,224),\n",
    "                                                   color_mode=\"rgb\",\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   batch_size=32)\n",
    "\n",
    "print(\"Classes and their Indices: \\n{a}\".format(a=train_set.class_indices)) # printing the classes with their numerical labels given by ImageDataGenerator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showDataDistribution():\n",
    "    \"\"\"\n",
    "    Extract the exact number of data samples in various sets,\n",
    "    are used for the model training, evaluation, and testing.\n",
    "    \"\"\"\n",
    "    plt.bar(['Train','Validation'],\n",
    "            [train_set.samples, validation_set.samples],\n",
    "            align='center',\n",
    "            color=['green','red']) # setting alignment of the text to the center\n",
    "                                            # here x will be ['Train','Validation','Test']\n",
    "                                            # y will be [train_set.samples, validation_set.samples, test_set.samples]\n",
    "                                            # colors of respective bars will be ['green','red','blue']\n",
    "    plt.ylabel('Number of Images') # setting the y-axis label\n",
    "    plt.title('Data Distribution') # giving title to the bar plot\n",
    "    for iteration in range(2): # putting tag of total number of data samples available in the various sets,\n",
    "                                # on the top of the bar with bounding box.\n",
    "        x = ['Train','Validation']\n",
    "        y = [train_set.samples,validation_set.samples]\n",
    "        plt.text(x=x[iteration],\n",
    "                 y=y[iteration],\n",
    "                 s=str(y[iteration]),\n",
    "                 ha='center', # setting horizontal-alignment to the 'center'\n",
    "                 bbox=dict(facecolor='yellow',alpha=0.8)) # here alpha is unknown, also mentioned in the documentation\n",
    "    plt.show()\n",
    "\n",
    "showDataDistribution() # calling the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "run = neptune.init_run(project='<your_neptune_username>/<project_name>',\n",
    "                       api_token=\"<api_token>\",\n",
    "                       source_files=['live_fruit_classification_without_preprocessing.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "project = neptune.init_project(project='<your_neptune_username>/<project_name>',\n",
    "                               api_token=\"<api_token>\")\n",
    "\n",
    "project['dataset/validation'].track_files('/path_to_validation_directory/validation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "model = neptune.init_model(project='<your_neptune_username>/<project_name>',\n",
    "                           api_token='<api_token>',\n",
    "                           name='CNN-1',\n",
    "                           key='MO1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = neptune.init_model_version(project='<your_neptune_username>/<project_name>',\n",
    "                                           api_token='<api_token>',\n",
    "                                           model='DLW-MO1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version['model_1/h5'].upload('/path_to_model_directory/_model_/fruit_classification_without_preprocessing.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size':32,\n",
    "          'learning_rate':0.001,\n",
    "          'epochs':6,\n",
    "          'steps_per_epoch':train_set.samples/train_set.batch_size,\n",
    "          'validation_steps':validation_set.samples/validation_set.batch_size,\n",
    "          'loss':'categorical_crossentropy',\n",
    "          'metrics':'accuracy'}\n",
    "\n",
    "run['training/model/parameters'] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "neptune_cbk = NeptuneCallback(run=run,\n",
    "                              base_namespace='train_metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 # setting the image dimension to the 224\n",
    "ker_init = \"he_normal\" # initializing the kernel initializer using the \"he_normal\" method because \"relu\" activation is used\n",
    "\n",
    "input_layer = Input(shape=(IMG_SIZE,IMG_SIZE,3)) # defining the global input layer\n",
    "\"\"\"\n",
    "Here every where padding value is \"same\", it means the is half padded,\n",
    "and this type of padding is called SAME type because output size is same\n",
    "as the input size when stride=1.\n",
    "\"\"\"\n",
    "conv1 = Conv2D(64, 3, activation='relu', kernel_initializer=ker_init, padding='same')(input_layer)\n",
    "conv2 = Conv2D(64, 3, activation='relu', kernel_initializer=ker_init, padding='same')(conv1)\n",
    "maxpool1 = MaxPool2D((2,2))(conv2)\n",
    "conv3 = Conv2D(128, 3, activation='relu', kernel_initializer=ker_init, padding='same')(maxpool1)\n",
    "conv4 = Conv2D(128, 3, activation='relu', kernel_initializer=ker_init, padding='same')(conv3)\n",
    "maxpool2 = MaxPool2D((2,2))(conv4)\n",
    "conv5 = Conv2D(256, 3, activation='relu', kernel_initializer=ker_init, padding='same')(maxpool2)\n",
    "conv6 = Conv2D(256, 3, activation='relu', kernel_initializer=ker_init, padding='same')(conv5)\n",
    "maxpool3 = MaxPool2D((2,2))(conv6)\n",
    "averagePooling = GlobalAveragePooling2D()(maxpool3)\n",
    "dense1 = Dense(64, activation='relu')(averagePooling)\n",
    "dense2 = Dense(64, activation='relu')(dense1)\n",
    "dense3 = Dense(32, activation='relu')(dense2)\n",
    "output_layer = Dense(6, activation='softmax')(dense3) # defining the global output layer with \"softmax\" activation\n",
    "\n",
    "model = Model(input_layer, output_layer) # defining the sequential model using Model class\n",
    "\n",
    "model.compile(loss=params['loss'], # using \"categorical_crossentropy\" due to multi-class problem\n",
    "              optimizer=Adam(learning_rate=params['learning_rate']), # setting learning rate to the 0.001, min(learning_rate) = high(performance)\n",
    "              metrics = CategoricalAccuracy(name=params['metrics'])) # using alias \"accuracy\" of CategoricalAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # getting alias of the all layers with their parameters and total number of trainable and non-trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model,\n",
    "           show_shapes=True, # showing the input and output shapes\n",
    "           show_dtype=True, # showing data types of layer\n",
    "           show_layer_names=True, # showing the name of the layer\n",
    "           rankdir='TB', # plot the graph in vertical manner (if 'LR' is passed instead of 'TB' then graph will plotted in horizontal manner)\n",
    "           expand_nested=True, # showing layers of cluster separately\n",
    "           show_layer_activations=True, # showing the activation function of the layer\n",
    "           dpi=128) # printing dots_per_inch {high value = high resolution}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the 3D Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "\"\"\"\n",
    "Here defaultdict is used because if a key is not found in the dictionary,\n",
    "then instead of a KeyError being thrown, a new entry is created.\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "\n",
    "font = ImageFont.truetype(font=\"/path_to_font_file/arial.ttf\", size=32)\n",
    "color_map = defaultdict(dict)\n",
    "color_map[Conv2D]['fill'] = 'orange'\n",
    "color_map[Dense]['fill'] = 'green'\n",
    "\n",
    "visualkeras.layered_view(model, legend=True, spacing=25, color_map=color_map, font=font, to_file='output_1.png').show()\n",
    "# show() is used to show the output on screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_set,\n",
    "                    validation_data=validation_set,\n",
    "                    steps_per_epoch=params['steps_per_epoch'],\n",
    "                    validation_steps=params['validation_steps'],\n",
    "                    epochs=params['epochs'],\n",
    "                    batch_size=params['batch_size'],\n",
    "                    callbacks=neptune_cbk) # setting the epochs to 6, means all batches are passed through network architecture upto 6 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/path_to_model_directory/_model_/fruit_classification_without_preprocessing.h5')\n",
    "# saving the model on the given location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = history.history # getting history of the performance of the model during training time\n",
    "\n",
    "training_accuracy=values['accuracy'] # training accuracy\n",
    "validation_accuracy=values['val_accuracy'] # validation accuracy\n",
    "\n",
    "training_loss=values['loss'] # training loss\n",
    "validation_loss=values['val_loss'] # validation loss\n",
    "\n",
    "epoch = range(len(training_accuracy)) # generating a list with length equal to the length of the \"training_accuracy\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# plotting the graph epoch vs training accuracy, epoch vs validation accuracy\n",
    "plt.plot(epoch,\n",
    "         training_accuracy,\n",
    "         label='training accuracy')\n",
    "plt.plot(epoch,\n",
    "         validation_accuracy,\n",
    "         label='validation acuuracy')\n",
    "plt.title('Training and validation accuracy') # giving the title to the plot\n",
    "plt.xlabel('epoch') # setting the xlabel\n",
    "plt.ylabel('accuracy') # setting the ylabel\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('/path_to_figures_directory/_figures_/training-validation_accuracy.png',\n",
    "            format='png',\n",
    "            dpi=400) # saving the figure with high dots_per_inch in \"png\" format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# plotting the graph epoch vs training accuracy, epoch vs validation loss\n",
    "plt.plot(epoch,\n",
    "         training_loss,\n",
    "         label='training loss')\n",
    "plt.plot(epoch,\n",
    "         validation_loss,\n",
    "         label='validation loss')\n",
    "plt.title('Traning and validation loss') # giving the title to the plot\n",
    "plt.xlabel('epoch') # setting the xlabel\n",
    "plt.ylabel('loss') # setting the ylabel\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('/path_to_figures_directory/_figures_/training-validation_loss.png',\n",
    "            format='png',\n",
    "            dpi=400) # saving the figure with high dots_per_inch in \"png\" format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing the model on the single image\n",
    "OR\n",
    "single input.\n",
    "\"\"\"\n",
    "IMG = cv2.imread('/path_to_test_directory/test/fb2.png') # reading an image from the given path in rgb mode\n",
    "RESIZE_IMG = cv2.resize(IMG, (224,224)) # resizing the image to the shape (224,224)\n",
    "PROCESSED_IMG = np.array(RESIZE_IMG).reshape(-1,224,224,3) # reshaping the image to the (-1,224,224,3) due to single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_prob = model.predict(PROCESSED_IMG) # giving processed image to the model \n",
    "print(\"Classwise probability: {a}\".format(a=prediction_prob))\n",
    "print(\"Classes: {b}\".format(b=train_set.class_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.stop()\n",
    "model_version.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment1-venv",
   "language": "python",
   "name": "environment1-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "neptune": {
   "notebookId": "98c53c90-163b-48e8-9d57-5f5243bc76f4",
   "projectVersion": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
